\section{Matrix Differentiation}
\subsection{The Box Product}

\begin{frame}
\frametitle{Derivatives of Linear Matrix-Matrix Functions}
\begin{enumerate}
\item A matrix--matrix derivative is a
  matrix outer-product:
$$\color{m1} F(\mX) = \trace(\mA\mX)\mB,\qquad\frac{\partial
    F}{\partial\mX} = \myvec(\mB^\top)\myvec^\top(\mA).
$$
\item 
A matrix--matrix derivative is a Kronecker product:
$$\color{m1} F(\mX) = \mA\mX\mB,\qquad\frac{\partial
    F}{\partial\mX} = \mA\otimes\mB^\top.
$$
\item
A matrix--matrix derivative \emph{cannot} be expressed
with standard operations.  We call the new operation the \alert{box
  product}
$$\color{m1} F(\mX) = \mA\mX^\top\mB,\qquad\frac{\partial
    F}{\partial\mX} = \mA\alert{\boxtimes}\mB^\top.
$$
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Direct Matrix Products}
A \alert{Direct Matrix Product}, $\color{m1}\mX=\mA\circledast\mB$, is
a matrix whose elements are
$$\color{m1}
x_{(i_1i_2)(i_3i_4)}=a_{i_{\sigma(1)}i_{\sigma(2)}}b_{i_{\sigma(3)}i_{\sigma(4)}}.
$$ 
\vspace{-.5cm}

\begin{itemize}
\item $\color{m1}(i_1i_2)$ is shorthand for $\color{m1}i_1n_2+i_2-1$
  where $\color{m1}i_2\in\{1,2,\ldots,n_2\}$.
\item  $\color{m1}\sigma$ is a permutation over $\color{m1}\mathbb{Z}_4$
\end{itemize}
The direct matrix products are central to matrix--matrix
differentiation.  8 can be expressed using Kronecker products:
$$
\color{m1}
\begin{array}{cccc}
\mA\otimes\mB     & \mA^\top\otimes\mB &
\mA\otimes\mB^\top & \mA^\top\otimes\mB^\top \\
\mB\otimes\mA     & \mB^\top\otimes\mA &
\mB\otimes\mA^\top & \mB^\top\otimes\mA^\top
\end{array}
$$
Then 8 using matrix outer products and the \alert{final 8 using the box product}.
\end{frame}

\begin{frame}
\frametitle{Definition}
Let $\color{m1}\mA\in\R^{m_1\times n_1}$ and
$\color{m1}\mB\in\R^{m_2\times n_2}$

\begin{definition}[Kronecker Product]
$\color{m1}\mA\otimes\mB\in\R^{(m_1m_2)\times(n_1n_2)}$
is defined by
{\color{m1}$(\mA\otimes\mB)_{(i-1)m_2+j,(k-1)n_2+l}=$ $a_{i\alert{k}}b_{j\alert{l}}=$ $(\mA\otimes\mB)_{(ij)(kl)}$}.
\end{definition}

\begin{definition}[Box Product]
$\color{m1}\mA\boxtimes\mB\in\R^{(m_1m_2)\times(n_1n_2)}$
is defined by
{\color{m1}
$(\mA\boxtimes\mB)_{(i-1)m_2+j,(k-1)n_1+l}=$ $a_{i\alert{l}}b_{j\alert{k}}=$ $(\mA\boxtimes\mB)_{(ij)(kl)}$.}
\end{definition}
\end{frame}

\begin{frame}
\frametitle{An example Kronecker and box product}
To see the structure consider the box product of two
$\color{m1}2\times 2$ matrices, $\color{m1}\mA$ and $\color{m1}\mB$:
\small
$$
\color{m1}
\begin{array}{cc}
\mA\otimes\mB & \mA\boxtimes\mB\\
\begin{pmatrix}
{\color<5>{OliveGreen}{a_{11}}}\color<1>{OliveGreen}{b_{11}} & {\color<6>{OliveGreen}{a_{11}}}\color<1>{OliveGreen}{b_{12}} & {\color<5>{OliveGreen}{a_{12}}}\color<2>{OliveGreen}{b_{11}} & {\color<6>{OliveGreen}{a_{12}}}\color<2>{OliveGreen}{b_{12}} \\
{\color<7>{OliveGreen}{a_{11}}}\color<1>{OliveGreen}{b_{21}} & {\color<8>{OliveGreen}{a_{11}}}\color<1>{OliveGreen}{b_{22}} & {\color<7>{OliveGreen}{a_{12}}}\color<2>{OliveGreen}{b_{21}} & {\color<8>{OliveGreen}{a_{12}}}\color<2>{OliveGreen}{b_{22}} \\
{\color<5>{OliveGreen}{a_{21}}}\color<3>{OliveGreen}{b_{11}} & {\color<6>{OliveGreen}{a_{21}}}\color<3>{OliveGreen}{b_{12}} & {\color<5>{OliveGreen}{a_{22}}}\color<4>{OliveGreen}{b_{11}} & {\color<6>{OliveGreen}{a_{22}}}\color<4>{OliveGreen}{b_{12}} \\
{\color<7>{OliveGreen}{a_{21}}}\color<3>{OliveGreen}{b_{21}} & {\color<8>{OliveGreen}{a_{21}}}\color<3>{OliveGreen}{b_{22}} & {\color<7>{OliveGreen}{a_{22}}}\color<4>{OliveGreen}{b_{21}} & {\color<8>{OliveGreen}{a_{22}}}\color<4>{OliveGreen}{b_{22}} \\
\end{pmatrix} 
& 
\begin{pmatrix}
{\color<5>{BrickRed}{a_{11}}}\color<1>{BrickRed}{b_{11}} & {\color<5>{BrickRed}{a_{12}}}\color<2>{BrickRed}{b_{11}} & {\color<6>{BrickRed}{a_{11}}}\color<1>{BrickRed}{b_{12}} & {\color<6>{BrickRed}{a_{12}}}\color<2>{BrickRed}{b_{12}} \\
{\color<7>{BrickRed}{a_{11}}}\color<1>{BrickRed}{b_{21}} & {\color<7>{BrickRed}{a_{12}}}\color<2>{BrickRed}{b_{21}} & {\color<8>{BrickRed}{a_{11}}}\color<1>{BrickRed}{b_{22}} & {\color<8>{BrickRed}{a_{12}}}\color<2>{BrickRed}{b_{22}} \\
{\color<5>{BrickRed}{a_{21}}}\color<3>{BrickRed}{b_{11}} & {\color<5>{BrickRed}{a_{22}}}\color<4>{BrickRed}{b_{11}} & {\color<6>{BrickRed}{a_{21}}}\color<3>{BrickRed}{b_{12}} & {\color<6>{BrickRed}{a_{22}}}\color<4>{BrickRed}{b_{12}} \\
{\color<7>{BrickRed}{a_{21}}}\color<3>{BrickRed}{b_{21}} & {\color<7>{BrickRed}{a_{22}}}\color<4>{BrickRed}{b_{21}} & {\color<8>{BrickRed}{a_{21}}}\color<3>{BrickRed}{b_{22}} & {\color<8>{BrickRed}{a_{22}}}\color<4>{BrickRed}{b_{22}} \\
\end{pmatrix} 
\end{array}
$$
\end{frame}

\begin{frame}
\begin{minipage}[b]{0.45\linewidth}
\centering
\small
\only<1>{
$$\color{m1}
\mA = 
\begin{pmatrix} 
1 & 1 & 1 & \cdots & 1 \\
2 & 2 & 2 & \cdots & 2 \\
3 & 3 & 3 & \cdots & 3 \\
\vdots & \vdots & \vdots & & \vdots\\
10 & 10 & 10 & \cdots & 10 \\
\end{pmatrix}
$$}
\only<2->{\pgfimage[width=3cm,height=3cm]{figs/A_1.png}}
\end{minipage}
\hspace{0.1cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\only<1>{
$$\color{m1}
\mB=
\begin{pmatrix} 
1 & 2 & 3 & \cdots & 10 \\
1 & 2 & 3 & \cdots & 10 \\
1 & 2 & 3 & \cdots & 10 \\
\vdots & \vdots & \vdots & & \vdots\\
1 & 2 & 3 & \cdots & 10 \\
\end{pmatrix}
$$}
\only<2->{\pgfimage[width=3cm,height=3cm]{figs/B_1.png}}
\end{minipage}
\only<3>{
 $$\color{m1}\mA\otimes\mB$$
\begin{center}
\pgfimage[height=3cm]{figs/kron_1}
\end{center}
}
\only<4>{
 $$\color{m1}\mA\boxtimes\mB$$
\begin{center}
\pgfimage[height=3cm]{figs/box_1}
\end{center}
}
\only<5>{
$$\color{m1}\myvec(\mA)\myvec^\top(\mB)$$
\begin{center}
\pgfimage[height=3cm]{figs/vec_1}
\end{center}
}


\end{frame}

\begin{frame}
The box product behaves similarly to the Kronecker product:
\begin{enumerate}
\item<1-> \textbf{Vector Multiplication}:
{\small
$$\color{m1}
\begin{array}{cc}
(\mB^\top\otimes\mA)\myvec(\mX)=\myvec(\mA\mX\mB) &
(\mB^\top\boxtimes\mA)\myvec(\mX)=\myvec(\mA\mX^{\alert{\top}}\mB)
\end{array}
$$}
\item<2-> \textbf{Matrix Multiplication}:
{\small
$$\color{m1}
\begin{array}{cc}
(\mA\otimes\mB)(\mC\otimes\mD)=(\mA\mC)\otimes(\mB\mD)&
(\mA\boxtimes\mB)(\mC\boxtimes\mD)=(\mA\alert{\mD})\alert{\otimes}(\mB\alert{\mC})
\end{array}
$$}
\item<3-> \textbf{Inverse and Transpose}:
{\small
$$\color{m1}
\begin{array}{cc}
(\mA\otimes\mB)^{-1}=\mA^{-1}\otimes\mB^{-1} &
(\mA\boxtimes\mB)^{-1}=\alert{\mB}^{-1}\boxtimes\alert{\mA}^{-1}\\
(\mA\otimes\mB)^{\top}=\mA^{\top}\otimes\mB^{\top} &
(\mA\boxtimes\mB)^{\top}=\alert{\mB}^{\top}\boxtimes\alert{\mA}^{\top}
\end{array}
$$}
\item<4-> \textbf{Mixed Products}:
{\small
$$\color{m1}
\begin{array}{cc}
(\mA\otimes\mB)(\mC\boxtimes\mD) =
  (\mA\alert{\mC})\boxtimes(\mB\alert{\mD}) &
(\mA\boxtimes\mB)(\mC\otimes\mD) =
  (\mA\alert{\mD})\boxtimes(\mB\alert{\mC})
\end{array}
$$}
\end{enumerate}
\end{frame}

\begin{frame}
Let $\color{m1}\mA\in\R^{m_1\times n_1}$, $\color{m1}\mB\in\R^{m_2\times n_2}$ then
\begin{itemize}
\item \textbf{Trace:}
$$\color{m1}
\trace(\mA\otimes\mB) =
\trace(\mA)\trace(\mB)\qquad\trace(\mA\boxtimes\mB) = \trace(\mA\mB)
$$
\item \textbf{Determinant:}  Here $m_1=n_1$ and $m_2=n_2$ is required
{
\color{m1}
\begin{eqnarray*}
\det(\mA\otimes\mB) &=&(\det(\mA))^{m_2}(\det(\mA))^{m_1}\\
\det(\mA\boxtimes\mB) &=&
\alert{(-1)^{{m_1 \choose 2}{m_2\choose 2}}}(\det(\mA))^{m_2}(\det(\mB))^{m_1}
\end{eqnarray*}}
\item \textbf{Associativity:}
$$\color{m1}
(\mA\otimes\mB)\otimes\mC=
\mA\otimes(\mB\otimes\mC) \qquad 
(\mA\boxtimes\mB)\boxtimes\mC=
\mA\boxtimes(\mB\boxtimes\mC),
$$
but not for mixed products.  In general we have
$$\color{m1}
(\mA\otimes\mB)\boxtimes\mC \alert{\neq}
\mA\otimes(\mB\boxtimes\mC) \qquad 
(\mA\boxtimes\mB)\otimes\mC \alert{\neq}
\mA\boxtimes(\mB\otimes\mC).
$$

\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Identity Box Products}
The identity box product $\color{m1}\mI_m\boxtimes\mI_n$ is
permutation matrix with interesting properties.  Let
$\color{m1}\mA\in\R^{m_1\times n_1}$, $\color{m1}\mB\in\R^{m_2\times
  n_2}$.
\begin{description}
\item[Orthonormal:]
$\color{m1}(\mI_m\boxtimes\mI_n)^\top(\mI_m\boxtimes\mI_n)=\mI_{mn}$
\item[Transposition:]
$\color{m1}(\mI_{m_1}\boxtimes\mI_{n_1})\myvec(\mA) = \myvec(\mA^\top)$
\item[Connector:]
Converting a Kronecker product to a box product:
$\color{m1}(\mA\otimes\mB)(\mI_{n_1}\boxtimes\mI_{n_2}) =
\mA\boxtimes\mB$.\\
Converting a box product to a Kronecker product:
$\color{m1}(\mA\boxtimes\mB)(\mI_{n_2}\boxtimes\mI_{n_1}) =
\mA\otimes\mB$.
\end{description}
\end{frame}
\begin{frame}
\frametitle{A Wolf in Sheep's Clothing}
Although the notation for box products is new,
$\color{m1}\mI_m\boxtimes\mI_n$ has long been known as
$\color{m1}\mT_{m,n}$ by physicists, or as a \alert{stride
  permutation} $\color{m1}\mL_m^{mn}$ by computer scientists, and a
\alert{perfect shuffle} by statisticians.  These objects are
identical, but the box-product allows us to express more complex
identities more compactly, e.g. let $\color{m1}\mA\in\R^{m_1\times
  n_1}$, $\color{m1}\mB\in\R^{m_2\times n_2}$ then
$$\color{m1}
(\mI_{m_1}\boxtimes\mI_{n_1m_2}\boxtimes\mI_{n_2})\myvec((\mA\otimes\mB)^\top)=\myvec(\myvec(\mA)\myvec^\top(\mB)).
$$
The initial permutation matrix would have to be written 
$$\color{m1}
\mI_{m_1}\boxtimes\mI_{n_1m_2}\boxtimes\mI_{n_2} = (\mI_{m_1}\otimes\mT_{n_1m_2,m_1})\mT_{m_1,n_1m_1m_2},
$$
if the notation of box-products wasn't being used.  
\end{frame}

\begin{frame}
\frametitle{The FFT matrix}
Direct matrix products are important because such matrices
can be multiplied fast with each other and with vectors.\\
\medskip
Let us show how the FFT can be done in terms of Kronecker and box products.  
\medskip
Recall that the DFT matrix of order $n$ is given by 
$\color{m1}
\mF_n = [e^{-2\pi i kl/n}]_{0\leq k,l<n}= [\omega_n^{kl}]_{0\leq k,l<n}
$
and therefore
$$\color{m1}
\mF_2 = \begin{pmatrix}1 & 1 \\ 1 & -1\end{pmatrix},\qquad
\mF_4 = \begin{pmatrix} 1 & 1 & 1 & 1 \\
                        1 & -i & -1 & i \\
                        1 & -1 & 1 & -1 \\
                        1 & i & -1 & -i
\end{pmatrix}
$$
%Also define $\color{m1}\mY_4=\begin{pmatrix}1 & 1 \\ 1 & -i\end{pmatrix}$.
\end{frame}

\begin{frame}
\frametitle{Fast Matrix Multiplication By Factorization}
We can factor the matrix $\color{m1}\mF_4$ as follows:

{\small
$$\color{m1}
\mF_4 = 
\begin{pmatrix}
1 &   & 1 &  \\
  & 1 &   & 1\\
1 &   & -1 &  \\
  & 1 &    & -1
\end{pmatrix}
\begin{pmatrix}
1 &   &   &  \\
  & 1 &   &  \\
  &   & 1 &  \\
  &   &   & -i\\
\end{pmatrix}
\begin{pmatrix}
1 & 1 &   &  \\
1 & -1 &   &  \\
  &   & 1 & 1\\
  &   & 1 & -1\\
\end{pmatrix}
\begin{pmatrix}
1 &   &   &  \\
  &   & 1 &  \\
  & 1 &   &  \\
  &   &   & 1\\
\end{pmatrix}
$$}
or more compactly
$$\color{m1}
\mF_4 = (\mF_2\otimes\mI_2)\diag\left(\myvec\begin{pmatrix}
1 & 1\\
1 & -i
\end{pmatrix}\right)
(\mI_2\boxtimes\mF_2)
$$
\end{frame}

\begin{frame}
\frametitle{The DFT factorization}
In general if we define the matrix 
$$\color{m1}
\mV_{N,M}(\alpha) = \begin{pmatrix} 
1 & 1 & 1 & \ldots & 1 \\
1 & \alpha & \alpha^2 & \ldots & \alpha^{M-1} \\
1 & \alpha^2 & \alpha^4 & \ldots & \alpha^{2(M-1)} \\
\vdots & \vdots & \vdots & \ddots & \vdots\\
1 & \alpha^{N-1} & \alpha^{2(N-1)} & \ldots & \alpha^{(N-1)(M-1)}
\end{pmatrix},
$$
For $\color{m1}N=km$ we have the following factorizations of the DFT matrix: 
{\color{m1}
\begin{eqnarray*}
\mF_{N} &=&
\alert{(\mF_k\otimes\mI_m)(\mathrm{diag}(\myvec(\mV_{m,k}(\omega_N))))(\mI_k\boxtimes\mF_m)}\\
\mF_{N} &=&
(\mF_m\boxtimes\mI_k)(\mathrm{diag}(\myvec(\mV_{m,k}(\omega_N))))(\mF_k\otimes\mI_m)
\end{eqnarray*}}
\end{frame}


\begin{frame}
This allows $\color{m1}\mathrm{FFT}_{N}(\vx)=\vy=\mF_{N}\vx$ to be computed as
$$\color{m1}
\mF_{N}\vx = \myvec((\mV_{m,k}(\omega_N)\circ(\mF_m\mX^\top))\mF_k^\top).
$$
where $\color{m1}\vx=\myvec(\mX)$ and $\color{m1}\circ$ denotes
elementwise multiplication ($\color{m1}.*$ in Matlab).
The direct computation has a cost of $\color{m1}\mathcal{O}(k^2m^2)$, whereas
the formula above does the job in $\color{m1}\mathcal{O}(km(k+m))$ operations.
Repeated use of the identity for $\color{m1}N=2^n$ leads to the Cooley-Tukey
FFT algorithm.  (James Cooley was at T. J. Watson from 1962 to 1991).\\
\medskip
The fastest FFT library in the world as of 2012 (\alert{Spiral}) uses
knowledge of several such factorizations to automatically 
optimize the FFT implementation for an arbitrary value of
$\color{m1}n$ to a given platform!
\end{frame}

\subsection{Derivative Rules}
\begin{frame}
\frametitle{Basic Differentiation Identities}
Let $\color{m1}\mX\in\R^{m\times n}$.
\begin{itemize}
\item \textbf{Identity}: $\color{m1}\mF(\mX)=\mX$, $\color{m1}\mF'(\mX)=\mI_{mn}$
\item \textbf{Transpose}:$\color{m1}\mF(\mX)=\mX^\top$, $\color{m1}\mF'(\mX)=\mI_{m}\boxtimes\mI_n$
\item \textbf{Chain Rule}: 
$$\color{m1}\mF(\mX)=\mG(\mH(\mX)),\quad\mF'(\mX)=\mG'(\mH(\mX))\mH'(\mX)$$
\item \textbf{Product Rule}: 
$$\color{m1}\mF(\mX)=\mG(\mX)\mH(\mX),\quad\mF'(\mX)=(\mI\otimes\mH^\top(\mX))\mG'(\mX)+(\mG(\mX)\otimes\mI)\mH'(\mX)$$
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{More Derivative Identitities}
Assume $\color{m1}\mX\in\R^{m\times m}$ is a square matrix.
\begin{itemize}
\item \textbf{Square:}
$\color{m1}\mF(\mX)=\mX^2$, $\color{m1}\mF'(\mX)=\mI_m\otimes\mX^\top+\mX\otimes\mI_m$.
\item \textbf{Inverse:} $\color{m1}\mF(\mX)=\mX^{-1}$, $\color{m1}\mF'(\mX)=-\mX^{-1}\otimes\mX^{-\!\top}$.
\item \textbf{+Transpose:} $\color{m1}\mF(\mX)=\mX^{-\!\top}$, $\color{m1}\mF'(\mX)=-\mX^{-\!\top}\boxtimes\mX^{-1}$.
\item \textbf{Square Root:} $\color{m1}\mF(\mX)=\mX^{1/2}$,
  $\color{m1}\mF'(\mX)=\left(\mI\otimes(\mX^{1/2})^\top+ \mX^{1/2}\otimes\mI \right)^{-1}$.
%\item \textbf{Exponential:}
% $\color{m1}\mF(\mX)=\exp(\mX)$,
%  $\color{m1}\mF'(\mX)=(e^\mX\otimes\mI-\mI\otimes
%  e^{\mX^\top})\left(\mX\otimes\mI-\mI\otimes\mX^\top\right)^{-1}$. \alert{new}
\item \textbf{Integer Power:} $\color{m1}\mF(\mX)=\mX^k$,
  $\color{m1}\mF'(\mX)=\sum_{i=0}^{k-1}\mX^i\otimes(\mX^{k-1-i})^{\top}$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Scalar-Matrix Derivatives}
First some simple identities:
$$\color{m1}
\begin{array}{ll}
f(\mX)=\trace(\mA\mX) & \qquad f'(\mX) = \mA^\top \\
f(\mX)=\trace(\mA\mX^\top) & \qquad f'(\mX) = \mA \\
f(\mX)=\logdet(\mX) & \qquad f'(\mX) = \mX^{-\!\top}
\end{array}
$$
Then the chain rule for more general expressions:
{\color{m1}
\begin{eqnarray*}
\myvec\left(\left(\frac{\partial}{\partial
  \mX}\logdet(\mG(\mX))\right)^\top\right) &=&
\left(\frac{\partial\mG}{\partial\mX}\right)^\top\myvec\left(\left(\mG(\mX)\right)^{-1}\right)\\
\myvec\left(\left(\frac{\partial}{\partial
  \mX}\trace(\mG(\mX))\right)^\top\right) &=&
\left(\frac{\partial\mG}{\partial\mX}\right)^\top\myvec(\mI)
\end{eqnarray*}}
\end{frame}

\begin{frame}
\frametitle{Human Scalar-Matrix Differentiation}
The chain rule to get scalar-matrix derivatives is awkward to use.
Instead we have some short-cuts.\\
\medskip
$$\color{m1}
\frac{\partial }{\partial
  \mX}\trace(\mG(\mX)\mH(\mX))=
\left.\frac{\partial}{\partial
  \mX}
\trace\left(\mH(\mY)\mG(\mX)+\mH(\mX)\mG(\mY)\right)\right|_{\mY=\mX},
$$
$$\color{m1}
\frac{\partial}{\partial \mX}
\trace(\mA\mF^{-1}(\mX))
= \left.-\frac{\partial}{\partial
  \mX}
\trace\left(\mF^{-1}(\mY)\mA\mF^{-1}(\mY)\mF(\mX)\right)\right|_{\mY=\mX},
$$
and
$$\color{m1}
\frac{\partial \logdet(\mF(\mX))}{\partial
  \mX} =
\left.\frac{\partial}{\partial
  \mX} \trace\left(\mF^{-1}(\mY)\mF(\mX)\right)\right|_{\mY=\mX}.
$$
\end{frame}

\begin{frame}
Finally if 
$$\color{m1}
r(x)=\frac{q(x)}{p(x)}=\frac{\sum_{i=1}^na_i x^i}{\sum_{j=1}^nb_ix^i}
$$
is a scalar-scalar function we can form a matrix-matrix function by simply
substituting a matrix $\color{m1}\mX$ for the scalar $\color{m1}x$.
Then 
$$\color{m1}
\frac{\partial}{\partial\mX}\trace(r(\mX)) = \left(r'(\mX)\right)^\top
$$
and if $\color{m1}h(x)=\log(r(x))$ then
$$\color{m1}
\frac{\partial}{\partial\mX}\logdet(r(\mX)) = \left(h'(\mX)\right)^\top.
$$
\end{frame}

\subsection{Forward and Reverse Mode Differentiation}
\begin{frame}
\centerline{Compute $\color{m1}f(\mX)=\trace(\mF(\mX))=\trace(((\mI+\mX)^{-1}\mX^\top)\mX)$}
\begin{center}
\begin{tikzpicture}
[scale=.7,transform shape,
matrix/.style={rectangle,draw=black,fill=blue!20,thick},
operation/.style={circle,draw=black,fill=red!20,thick},
arrow/.style={->,thick}]
\node[matrix] (F) {$\trace(\mF(\mX))$};
\node[left=5cm of F] (upperLeft) { };
\node[right=2cm of F] (upperRight) { };
\node [right,left=2pt of F]  (tmpF) at (F.west)
{
\only<7->{$\color{blue}f=\trace(\mT_5)$}
};\node[operation] (root) [below=of F] {$\times$}
edge [arrow] (F);
\node [right,left=2pt of root] (tmpRoot)
{
\only<6->{$\color{blue}\mT_5=\mT_4*\mX$}
};
\node[operation] (t4) [below left=of root] {$\times$}
edge [arrow] (root);
\node [left,align=center,red] (tmp4) at (t4.west)
{
\only<5->{$\color{blue}\mT_4=\mT_2*\mT_3$}
};
\node[matrix] (x3) [below right=of root] {$\mX$}
edge [arrow] (root);
\node[operation] (t2) [below left=of t4] {$(\cdot)^{-1}$}
edge [arrow] (t4);
\node [left,align=center,red] (tmp2) at (t2.west)
{
\only<3->{$\color{blue}\mT_2=\mT_1^{-1}$}
};
\node[operation] (t3) [below right=of t4] {$(\cdot)^\top$}
edge [arrow] (t4);
\node [left=2pt of t3]
{
\only<4->{$\color{blue}\mT_3=\mX^\top$}
};
\node[operation] (t1) [below=of t2] {$+$}
edge [arrow] (t2);
\node[matrix] (x2) [below=of t3] {$\mX$}
edge [arrow] (t3);
\node[matrix] (x1) [below right=of t1] {$\mX$}
edge [arrow] (t1);
\node[matrix] (i) [below left=of t1] {$\mI$}
edge [arrow] (t1);
\node [left,align=center,red] (textt1) at (t1.west)
{ \only<2->{$\color{blue}\mT_1=\mI+\mX$} };
\begin{pgfonlayer}{background}
\node [inner sep=.1cm, fill=green!10, fit=(i) (x3) (F) (upperLeft) (upperRight)] {};
\end{pgfonlayer}
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\vbox to .3cm {
\only<1>{\centerline{Forward mode computation of
    $\color{m1}f'(\mX)$:}}
\only<2>{\centerline{$\color{m1}(\mG+\mH)'=\mG'+\mH'$}}
\only<3>{\centerline{Chain rule:
    $\color{m1}(\mT_1^{-1})'=-(\mT_1^{-1}\otimes\mT_1^{-\!\top})\mT_1'$ }}
\only<4>{\centerline{$\color{m1}(\mX^\top)'=\mI\boxtimes\mI$}}
\only<5-6>{\centerline{Product rule:
    $\color{m1}(\mG\mH)'=(\mI\otimes\mH^\top)\mG'+(\mG\otimes\mI)\mH'$}} 
\only<7>{\centerline{$\color{m1}((\trace(\mF))')^\top=\myvec^{-1}\left(\myvec^\top(\mI)\mF'\right)$}}
}
\begin{center}
\begin{tikzpicture}
[scale=.68,transform shape,
matrix/.style={rectangle,draw=black,fill=blue!20,thick},
operation/.style={circle,draw=black,fill=red!20,thick},
arrow/.style={->,thick}]
\node[matrix] (F) {$\trace(\mF(\mX))$};
\node[left=7cm of F] (upperLeft) { };
\node[right=2cm of F] (upperRight) { };
\node[operation] (root) [below=of F] {$\times$}
edge [arrow] (F);
\node [left=2pt of F] (f6) {
\only<7->{\color{Sepia}\alert<7>{
$f'=\myvec^{-1}\left(\myvec^\top(\mI)\mT_5'\right)$
}}};
\node [right=2pt of root] (tmpRoot)
{ $\color{blue}\mT_5$ };
\node [left=2pt of root] (f5) {
\only<6->{\color{Sepia}\alert<6>{
$\mT_5'=(\mI\otimes\mX^\top)\mT_4'+(\mT_4\otimes\mI)(\mI\otimes\mI)$
}}};
\node[operation] (t4) [below left=of root] {$\times$}
edge [arrow] (root);
\node [right=2pt of t4] (tmp4)
{ $\color{blue}\mT_4$ };
\node [left=2pt of t4] (f4) {
\only<5->{\color{Sepia}\alert<5>{
$\mT_4'=(\mI\otimes\mT_3^\top)\mT_2'+(\mT_2\otimes\mI)\mT_3'$
}}};
\node[matrix] (x3) [below right=of root] {$\mX$}
edge [arrow] (root);
\node [left=2pt of x3] { \color{Sepia}\alert<1-5>{$\mI\otimes\mI$} };
\node[operation] (t2) [below left=of t4] {$(\cdot)^{-1}$}
edge [arrow] (t4);
\node [right=2pt of t2]
{ $\color{blue}\mT_2$ };
\node [left=2pt of t2] (f2) { 
  \only<3->{\color{Sepia}\alert<3-4>{$\mT_2'=-(\mT_2\otimes\mT_2^\top)\mT_1'$}} 
};
\node[operation] (t3) [below right=of t4] {$(\cdot)^\top$}
edge [arrow] (t4);
\node [left=2pt of t3]
{ $\color{blue}\mT_3$ };
\node [right=2pt of t3] (f3) {
  \only<4->{\color{Sepia}\alert<4>{ $\mT_3'=\mI\boxtimes\mI$ }}
};
\node[operation] (t1) [below=of t2] {$+$}
edge [arrow] (t2);
\node[matrix] (x2) [below=of t3] {$\mX$}
edge [arrow] (t3);
\node [left=2pt of x2] { \color{Sepia}\alert<1-3>{$\mI\otimes\mI$} };
\node [right=2pt of t1]
{ $\color{blue}\mT_1$ };
\node [left=2pt of t1] (f1) {
  \only<2->{\color{Sepia}\alert<2>{$\mT_1'=\m0+\mI\otimes\mI$}} 
};
\node[matrix] (x1) [below right=of t1] {$\mX$}
edge [arrow] (t1);
\node [left=2pt of x1] { \color{Sepia}\alert<1>{$\mI\otimes\mI$} };
\node[matrix] (i) [below left=of t1] {$\mI$}
edge [arrow] (t1);
\node [left=2pt of i] (n0) { \color{Sepia}\alert<1>{$\m0$} };
\begin{pgfonlayer}{background}
\node [inner sep=.1cm, fill=green!10, fit=(i) (x3) (F) (n0) (upperLeft) (upperRight)] {};
\end{pgfonlayer}
\end{tikzpicture}
\end{center}
\end{frame}


\begin{frame}
Forward mode differentiation
\begin{itemize}
\item Requires huge intermediate matrices.
\item Only the last step reduces the size.
\end{itemize}
\textbf{Critical points:} 
\begin{itemize}
\item $\color{m1}\mF'(\mX)$ is composed of Kronecker, box (and outer)
  products for a large class of functions.  \alert{(wow!)}
\item These can be ``unwound'' by multiplication with vectorized
  scalar-matrix derivatives \alert{(gasp!)}:
$$\color{m1}
\myvec^\top(\mC)\mA\otimes\mB = \myvec^\top(\mB^\top\mC\mA)
$$
\end{itemize}
Reverse mode differentiation:
\begin{itemize}
\item Evaluate the derivative from top to bottom.
\item Small scalar-matrix derivatives are propagated down the tree.
\end{itemize}
\end{frame}



\begin{frame}
\vbox to .3cm {
\only<1>{\centerline{Reverse Mode Differentiation: $\color{m1}(f')^\top=\myvec^{-1}\left(\myvec^\top(\mI)\mF'\right)$}}
\only<2>{
\centerline{$\color{m1}\myvec^\top(\mR_0)(\mI\otimes\mX^\top)=\myvec^\top(\alert<2>{\mX\mR_0\mI})$ }
}
\only<3>{
\centerline{$\color{m1}\myvec^\top(\mR_0)(\mT_4\otimes\mI)=\myvec^\top(\alert{\mI\mR_0\mT_4})$ }
}
\only<4>{
\centerline{$\color{m1}\myvec^\top(\mR_1)(\mI\otimes\mT_3^\top)=\myvec^\top(\alert{\mT_3\mR_1\mI})$ }
}
\only<5>{
\centerline{$\color{m1}\myvec^\top(\mR_1)(\mT_2\otimes\mI)=\myvec^\top(\alert{\mI\mR_1\mT_2})$ }
}
\only<6>{
\centerline{$\color{m1}\myvec^\top(\mR_3)(-\mT_2\otimes\mT_2^\top)=\myvec^\top(\alert{-\mT_2\mR_3\mT_2})$ }
}
\only<7>{
\centerline{$\color{m1}\myvec^\top(\mR_4)(\mI\boxtimes\mI)=\myvec^\top(\alert{\mI\mR_4^\top\mI})$ }
}
\only<8>{
\centerline{$\color{m1}\myvec^\top(\mR_5)\m0=\myvec(\m0)$ }
}
\only<9>{
\centerline{$\color{m1}\myvec^\top(\mR_5)\mI\otimes\mI=\myvec(\mR_5)$ }
}
\only<10>{
\centerline{$\color{m1}f'(\mX)=\mR_2^\top+\mR_6^\top+\mR_8^\top$ }
}
}
\begin{center}
\begin{tikzpicture}
[scale=.68,transform shape,
matrix/.style={rectangle,draw=black,fill=blue!20,thick},
operation/.style={circle,draw=black,fill=red!20,thick},
arrow/.style={->,thick}]
\node[matrix] (F) {$\trace(\mF(\mX))$};
\node[left=8cm of F] (upperLeft) { };
\node[right=4cm of F] (upperRight) { };
\node[operation] (root) [below=of F] {$\times$}
edge [arrow] (F);
\node [left=1.5cm of F] (f6) {\color{OliveGreen}$f'=$
\only<3->{\alert<3>{$\mR_2^\top$}}
\only<7->{\alert<7>{$+\mR_6^\top$}}
\only<9->{\alert<9>{$+\mR_8^\top$}}
};
\node [right=2pt of F] (r0) {\alert<1-3>{$\mR_0=\mI$}};
\node [right=2pt of root] (tmpRoot)
{ $\color{blue}\mT_5$ };
\node [left=2pt of root] (f5) {
  \only<2-3>{\color{Sepia}$\mT_5'=$
    \alert<2>{$(\mI\otimes\mX^\top)$}
    \color{Sepia}$\mT_4'+$
    \alert<3>{$\mT_4\otimes\mI$}
  }
};
\node[operation] (t4) [below left=of root] {$\times$}
edge [arrow] (root);
\node [right=2pt of t4] (tmp4)
{ $\color{blue}\mT_4$ };
\node [above left=2pt of t4.north east] (r1) {
  \only<2-5> { \color{OliveGreen}\alert<2,4-5>{$\mR_1=\mX\mR_0$} } 
};
\node [left=2pt of t4] (f4) {
\only<4-5>{
\color{Sepia}$\mT_4'=$
\alert<4>{$(\mI\otimes\mT_3^\top)$}
\color{Sepia}$\mT_2'+$
\alert<5>{$(\mT_2\otimes\mI)$}
\color{Sepia}$\mT_3'$
}};
\node[matrix] (x3) [below right=of root] {$\mX$}
edge [arrow] (root);
\node [right=2pt of x3] {
  \only<3->{\color{OliveGreen}
    \alert<3>{$\mR_2=\mR_0\mT_4$}
  }
 };
\node[operation] (t2) [below left=of t4] {$(\cdot)^{-1}$}
edge [arrow] (t4);
\node [right=2pt of t2]
{ $\color{blue}\mT_2$ };
\node [above left=5pt of t2.north east] (r3) {
  \only<4-6> { \color{OliveGreen}\alert<4,6>{$\mR_3=\mT_3\mR_1$} } 
};
\node [left=2pt of t2] (f2) { 
  \only<6>{$\mT_2'=\alert{-(\mT_2\otimes\mT_2^\top)}\mT_1'$}
};
\node[operation] (t3) [below right=of t4] {$(\cdot)^\top$}
edge [arrow] (t4);
\node [left=2pt of t3]
{ $\color{blue}\mT_3$ };
\node [above right=5pt of t3.north west] (r3) {
  \only<5-7> { \color{OliveGreen}\alert<5,7>{$\mR_4=\mR_1\mT_2$} } 
};
\node [right=2pt of t3] (f3) {
  \only<7>{ $\mT_3'=\alert{\mI\boxtimes\mI}$ }
};
\node[operation] (t1) [below=of t2] {$+$}
edge [arrow] (t2);
\node[matrix] (x2) [below=of t3] {$\mX$}
edge [arrow] (t3);
\node [right=2pt of x2] (r3) {
  \only<7-> { \color{OliveGreen}\alert<7>{$\mR_6=\mR_4^\top$} } 
};
\node [right=2pt of t1] { $\color{blue}\mT_1$ };
\node [above left=5pt of t1.north east] (r5) {
  \only<6-9> { \color{OliveGreen}\alert<6,8-9>{$\mR_5=-\mT_2\mR_3\mT_2$} } 
};
\node [left=2pt of t1] (f1) {
  \only<8-9>{$\mT_1'=$\alert<8>{$\m0$}+\alert<9>{$\mI\otimes\mI$}}
};
\node[matrix] (x1) [below right=of t1] {$\mX$}
edge [arrow] (t1);
\node [right=2pt of x1] (r8) {
  \only<9-> { \color{OliveGreen}\alert<9>{$\mR_8=\mR_5$} } 
};
\node[matrix] (i) [below left=of t1] {$\mI$}
edge [arrow] (t1);
\node [right=2pt of i] (r7) {
  \only<8-9> { \color{OliveGreen}\alert<8>{$\mR_7=\m0$} } 
};
\begin{pgfonlayer}{background}
\node [inner sep=.1cm, fill=green!10, fit=(i) (x3) (F) (n0) (upperLeft) (upperRight)] {};
\end{pgfonlayer}
\end{tikzpicture}
\end{center}
\end{frame}

\section{Example AMD Code}
\begin{frame}
\frametitle{Our Automatic Matrix Differentiation Library}
\begin{itemize}
\item Provide both \alert{symbolic} and \alert{algorithmic} differentiation.
\begin{itemize}
\item Symbolic differentiation gives results as Matlab code.
\item Algorithmic differentiation supports multi-threading, but requires the Elemental matrix library.
\end{itemize}
\item Support operations $\color{m1}+$, $\color{m1}-$, $\color{m1}*$, $\color{m1}.*$, {\color{m1}transpose}, {\color{m1}inverse}, {\color{m1}trace} and {\color{m1}log-determinant}. 
\item Two ways to compute symbolic differentiation: 
      as a \texttt{C++} program and on command line.
\end{itemize}
\end{frame}
\subsection{A Derivative}
\begin{frame}[fragile]
\frametitle{Example AMD Program}
Define a Scalar-Matrix function $\color{m1} \trace(\mA\mX\mB\mX)$
\footnotesize
\begin{semiverbatim}
  \color{red}/** Create a variable X and an identity function */\color{black}
  symbolic_matrix_type X("X", ROW, COL);
  SymbolicMMFunc fX(X, false);
  \color{red}/** Create constants A,B and corresponding constant functions */\color{black}
  symbolic_matrix_type A("A", ROW, COL);
  symbolic_matrix_type B("B", ROW, COL);
  SymbolicMMFunc fA(A, true);
  SymbolicMMFunc fB(B, true);
  \color{red}/** Scalar-matrix function placeholder */\color{black}
  SymbolicSMFunc func; 
  func = trace(fA*fX*fB*fX);
  std::cout << "Function Value:   " << func.functionVal.getString() 
            << std::endl;
  std::cout << "Derivative Value: " << func.derivativeVal.getString()
            << std::endl;
\end{semiverbatim}
\end{frame}

\begin{frame}[fragile]
The resulting derivative is:

\begin{semiverbatim}
./SymbolicSample.exe
Function Value:   trace(((A*X)*B)*X)
Derivative Value: (((B*X)*A)'+((A*X)*B)')
\end{semiverbatim}

\end{frame}

\subsection{A Taylor Expansion}
\begin{frame}
\frametitle{Taylor expansion}
Recall that for a scalar-scalar function the Taylor expansion of a
function $\color{m1}f$ around a value $\color{m1}x_0$ looks like:
$$\color{m1}
f(x) = f(x_0)+(x-x_0)f'(x_0)+\frac{1}{2}(x-x_0)^2f''(x_0)+\frac{1}{3!}(x-x_0)^3f'''(x_0)+\cdots
$$
For a vector valued function $\color{m1}f'(\vx_0)$ is a vector and $\color{m1}f''(\vx_0)$
is a matrix, so the Taylor expansion looks like
$$\color{m1}
f(\vx) = f(\vx_0)+(\vx-\vx_0)^{\top}f'(\vx_0)+\frac{1}{2}(\vx-\vx_0)^{\top} f''(\vx_0)(\vx-\vx_0)+\cdots
$$
The third derivative here is a 3-dimensional tensor, and it's a bit
unclear what the best way to continue the expansion is.  For
scalar-matrix functions it's even messier.
\end{frame}

\begin{frame}
\frametitle{Taylor expansion for scalar-matrix functions}
For a matrix-matrix function like $\color{m1} F(\mX)=(\mI-\mX)^{-1}$ around $\color{m1}\mX_0=0$
we can pretend $\color{m1}\mX$ is a scalar and get
$$\color{m1}
(\mI-\mX)^{-1}= \mI+\mX+\mX^2+\mX^3+\cdot
$$
Thus for a scalar-matrix function like $\color{m1}f(\mX)=\trace((\mI-\mX)^{-1})$ we can
write the Taylor expansion as
$$\color{m1}
\trace((\mI-\mX)^{-1})= \trace(\mI)+\trace(\mX)+\trace(\mX^2)+\trace(\mX^3)+\cdot,
$$
although we do not explicitly compute $\color{m1}f'(\mX), f''(\mX), f'''(\mX),\ldots$
we compute individual terms of the Taylor expansion efficiently.

But we can compute $\color{m1}f''(\mX)vec(\mX)$ efficiently using our code.  This allows 
us to automate the entire process of building the Taylor expansion.
\end{frame}

\begin{frame}[fragile]
\frametitle{Taylor expansion for $\logdet(\mX)$}
We use these matrix differentiation rules to compute the Taylor expansion around the point $\color{m1}\mX_0$.  
Our code gives the following Taylor expansion for $\color{m1}\logdet(\mX)$ as seen in the code on the next two slides.
\begin{equation*}
\begin{split}
\color{m1} \logdet(\mX) &= \color{m1}\logdet(\mX_0) + \trace((\mX-\mX_0)\mX_0^{-1}) \\
         &\color{m1}+\frac{1}{2}\trace(-(\mX-\mX_0)\mX_0^{-1}(\mX-\mX_0)\mX_0^{-1}) \\
         &\color{m1}+ \frac{1}{3}\trace((\mX-\mX_0)\mX^{-1}(\mX-\mX_0)\mX^{-1}(\mX-\mX_0)\mX^{-1})+\cdots
\end{split}
\end{equation*}
\end{frame}

\begin{frame}[fragile]
\frametitle{Taylor expansion for $\logdet(\mX)$ continued}
If we define $\color{m1}\mDelta=(\mX-\mX_0)$ to be a constant function (with respect to $\color{m1}\mX_0$)
we can compute the Taylor series iteratively as follows:
\begin{equation*}
\color{m1}f_0(\mX_0) = \logdet(\mX_0) 
\end{equation*}
\begin{equation*}
\color{m1}f_1(\mX_0) = \trace(\mDelta(f_0'(\mX_0))^{T})
\end{equation*}
\begin{equation*}
\color{m1}f_2(\mX_0) = \trace(\mDelta(f_1'(\mX_0))^{T})
\end{equation*}
\begin{equation*}
\color{m1}f_3(\mX_0) = \trace(\mDelta(f_2'(\mX_0))^{T})
\end{equation*}
The Taylor expansion can now be written:
$$
\color{m1} \logdet(\mX) = f_0(\mX_0) + f_1(\mX_0) + \frac{1}{2!}f_2(\mX_0) + \frac{1}{3!}f_3(\mX_0) \cdots
$$
\end{frame}

\begin{frame}[fragile]
\frametitle{Taylor expansion for $\logdet(\mX)$ continued} 
\scriptsize
\begin{semiverbatim}
  \alert{// Initialize matrices}
  symbolic_matrix_type X("X", ROW, COL);
  symbolic_matrix_type X0("X0", ROW, COL);
  symbolic_matrix_type Delta("(X-X0)", ROW, COL);
  AMD::SymbolicScalarMatlab a2("1/2!");
  AMD::SymbolicScalarMatlab a3("1/3!"); 
  SymbolicSMFunc r2(a2,ROW,COL);
  SymbolicSMFunc r3(a3,ROW, COL);
  \alert{// Define Matrix-Matrix functions}
  SymbolicMMFunc fX(X, false);
  SymbolicMMFunc fX0(X0, false);
  SymbolicMMFunc fDelta(Delta, true);
  \alert{// Compute Taylor series terms iteratively} 
  SymbolicSMFunc f0 =  logdet(fX0);
  SymbolicSMFunc f1 = trace(fDelta * transpose(*f0.derivativeFuncVal));
  SymbolicSMFunc f2 = trace(fDelta * transpose(*f1.derivativeFuncVal));
  SymbolicSMFunc f3 = trace(fDelta * transpose(*f2.derivativeFuncVal));
  \alert{// Taylor Expansion with four terms}
  SymbolicSMFunc func = f0 + f1 + r2*f2 + r3*f3;
\end{semiverbatim}
\end{frame}

\begin{frame}[fragile]
Here is the result:
\scriptsize
\begin{semiverbatim}
./TaylorSample.exe
The first 4 terms of Taylor Expansion for logdet(X) around X0 is:
(((log(det(X0))
 +trace((X-X0)*inv(X0)))
 +(1/2!*trace((X-X0)*(-((inv(X0)*(X-X0))*inv(X0))))))
 +(1/3!*trace((X-X0)*( (-((inv(X0)*((X-X0)*(inv(X0)*(-(X-X0)))))*inv(X0)))'
                      +(-((inv(X0)*((-(X-X0))*(inv(X0)*(X-X0))))*inv(X0)))')')))
\end{semiverbatim}
\end{frame}

\begin{frame}
\frametitle{Conclusion} 

\begin{itemize}
\item Dense numerical differentiation using \texttt{libElemental}.

  \begin{itemize}
  \item Currently, we support multi-threading.
  \item Support for distributed computing planned.
  \end{itemize}

\item Plan to support sparse matrices and to provide python bindings
  (numpy library).
\item To obtain AMD: (\alert{\url{https://github.com/pkambadu/AMD}})
\end{itemize}
\end{frame}



\end{document}
